# Enhanced RAG System Architecture with Graph RAG

## Table of Contents
1. [System Overview](#system-overview)
2. [Core Architecture](#core-architecture)
3. [Graph RAG Integration](#graph-rag-integration)
4. [Agent System Design](#agent-system-design)
5. [Implementation Methodology](#implementation-methodology)
6. [Deployment Strategy](#deployment-strategy)
7. [Testing & Validation](#testing-validation)

---

## System Overview

### Architecture Principles
- **Hybrid Retrieval**: Vector + Sparse + Graph-based retrieval
- **Event-Driven**: Asynchronous agent communication
- **Graph-Native**: Neo4j for knowledge graph and relationship traversal
- **Production-Ready**: HA, monitoring, security, scalability
- **Self-Improving**: Continuous evaluation and optimization

### Technology Stack
```yaml
Storage Layer:
  - Vector DB: Milvus / Qdrant / Weaviate
  - Graph DB: Neo4j Enterprise/Community
  - Metadata DB: PostgreSQL
  - Cache: Redis

Processing Layer:
  - Embedding: sentence-transformers, OpenAI
  - LLM: Local (Llama2, Mistral) or API (GPT-4, Claude)
  - NLP: spaCy, NLTK for entity extraction
  - Graph Processing: Neo4j Graph Data Science

Infrastructure:
  - Orchestration: Docker Compose / Kubernetes
  - Message Bus: RabbitMQ / Kafka
  - Monitoring: Prometheus + Grafana
  - Tracing: Jaeger / OpenTelemetry
```

---

## Core Architecture

### System Diagram
```
┌─────────────────────────────────────────────────────────────────┐
│                         User Interface                           │
│                    (Web UI / CLI / API)                          │
└────────────────────────────┬────────────────────────────────────┘
                             │
┌────────────────────────────▼────────────────────────────────────┐
│                       API Gateway                                │
│            (Auth, Rate Limiting, Load Balancing)                 │
└────────┬─────────────────┬─────────────────┬────────────────────┘
         │                 │                 │
         ▼                 ▼                 ▼
┌────────────────┐ ┌──────────────┐ ┌──────────────────┐
│ Control Plane  │ │  Event Bus   │ │  State Store     │
│  Orchestrator  │ │  (RabbitMQ)  │ │  (PostgreSQL)    │
└────────┬───────┘ └──────┬───────┘ └──────────────────┘
         │                │
         ▼                ▼
┌─────────────────────────────────────────────────────────┐
│                   Agent Ecosystem                        │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐│
│  │ Planner  │  │ Analyzer │  │  Builder │  │ Indexer ││
│  └──────────┘  └──────────┘  └──────────┘  └─────────┘│
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐│
│  │Retriever │  │ Generator│  │ Verifier │  │Optimizer││
│  └──────────┘  └──────────┘  └──────────┘  └─────────┘│
└─────────┬───────────────────────────────────────────────┘
          │
          ▼
┌─────────────────────────────────────────────────────────┐
│                  Storage Layer                           │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐│
│  │ Vector DB│  │ Neo4j    │  │PostgreSQL│  │  Redis  ││
│  │ (Milvus) │  │ (Graph)  │  │(Metadata)│  │ (Cache) ││
│  └──────────┘  └──────────┘  └──────────┘  └─────────┘│
└─────────────────────────────────────────────────────────┘
```

### Enhanced Control Plane

```python
# control_plane/orchestrator.py

import asyncio
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class WorkflowStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"

@dataclass
class WorkflowState:
    workflow_id: str
    status: WorkflowStatus
    current_step: str
    context: Dict[str, Any]
    checkpoints: List[Dict]
    created_at: datetime
    updated_at: datetime

class ControlPlaneOrchestrator:
    """
    Event-driven orchestrator for RAG system
    Manages agent lifecycle and workflow execution
    """
    
    def __init__(self, event_bus, state_store, agent_registry):
        self.event_bus = event_bus
        self.state_store = state_store
        self.agent_registry = agent_registry
        self.workflows: Dict[str, WorkflowState] = {}
        
        # Subscribe to agent events
        self.event_bus.subscribe('agent.completed', self._handle_agent_completion)
        self.event_bus.subscribe('agent.failed', self._handle_agent_failure)
        
    async def execute_workflow(self, workflow_type: str, inputs: Dict) -> str:
        """
        Execute a workflow with checkpointing and recovery
        """
        workflow_id = self._generate_workflow_id()
        
        # Initialize workflow state
        workflow = WorkflowState(
            workflow_id=workflow_id,
            status=WorkflowStatus.PENDING,
            current_step="init",
            context=inputs,
            checkpoints=[],
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        self.workflows[workflow_id] = workflow
        await self.state_store.save_workflow(workflow)
        
        # Execute workflow steps
        try:
            if workflow_type == "indexing":
                await self._execute_indexing_workflow(workflow)
            elif workflow_type == "query":
                await self._execute_query_workflow(workflow)
            elif workflow_type == "optimization":
                await self._execute_optimization_workflow(workflow)
        except Exception as e:
            await self._handle_workflow_failure(workflow_id, e)
            
        return workflow_id
    
    async def _execute_indexing_workflow(self, workflow: WorkflowState):
        """
        Indexing workflow: Analyze → Chunk → Embed → Graph Extract → Store
        """
        steps = [
            ("analyze", "analyzer_agent"),
            ("chunk", "chunker_agent"),
            ("embed", "embedding_agent"),
            ("extract_graph", "graph_extractor_agent"),
            ("store_vectors", "vector_store_agent"),
            ("store_graph", "graph_store_agent"),
            ("verify", "verifier_agent")
        ]
        
        for step_name, agent_name in steps:
            workflow.current_step = step_name
            workflow.status = WorkflowStatus.RUNNING
            await self.state_store.save_workflow(workflow)
            
            # Execute agent
            agent = self.agent_registry.get(agent_name)
            result = await agent.execute(workflow.context)
            
            # Update context with results
            workflow.context[f"{step_name}_result"] = result
            
            # Create checkpoint
            checkpoint = {
                "step": step_name,
                "timestamp": datetime.utcnow(),
                "context_snapshot": workflow.context.copy()
            }
            workflow.checkpoints.append(checkpoint)
            
            # Emit progress event
            await self.event_bus.publish("workflow.progress", {
                "workflow_id": workflow.workflow_id,
                "step": step_name,
                "progress": (steps.index((step_name, agent_name)) + 1) / len(steps)
            })
        
        workflow.status = WorkflowStatus.COMPLETED
        await self.state_store.save_workflow(workflow)
    
    async def _execute_query_workflow(self, workflow: WorkflowState):
        """
        Query workflow: Analyze Query → Hybrid Retrieve → Rerank → Generate
        """
        steps = [
            ("analyze_query", "query_analyzer_agent"),
            ("vector_retrieve", "vector_retriever_agent"),
            ("graph_retrieve", "graph_retriever_agent"),
            ("sparse_retrieve", "sparse_retriever_agent"),
            ("fusion", "fusion_agent"),
            ("rerank", "reranker_agent"),
            ("generate", "generator_agent"),
            ("evaluate", "evaluator_agent")
        ]
        
        for step_name, agent_name in steps:
            workflow.current_step = step_name
            agent = self.agent_registry.get(agent_name)
            result = await agent.execute(workflow.context)
            workflow.context[f"{step_name}_result"] = result
        
        workflow.status = WorkflowStatus.COMPLETED
        await self.state_store.save_workflow(workflow)
```

---

## Graph RAG Integration

### Why Graph RAG?

**Traditional Vector RAG Limitations:**
1. **Lost Relationships**: Cannot capture entity relationships
2. **Context Isolation**: Each chunk is independent
3. **No Reasoning Paths**: Cannot trace connections between concepts
4. **Limited Multi-Hop**: Struggles with questions requiring multiple reasoning steps

**Graph RAG Benefits:**
1. **Relationship Awareness**: Explicitly models connections
2. **Multi-Hop Reasoning**: Traverses graph paths
3. **Entity Disambiguation**: Resolves entity references
4. **Contextual Enrichment**: Adds structural context to chunks
5. **Explainable Retrieval**: Shows reasoning paths

### Graph RAG Architecture

```
Document Processing Pipeline:
┌──────────────┐
│   Document   │
└──────┬───────┘
       │
       ▼
┌──────────────────────────────────────┐
│  Entity & Relationship Extraction    │
│  - NER (spaCy, GLiNER)              │
│  - Coreference Resolution            │
│  - Relation Extraction (OpenIE)      │
└──────┬───────────────────────────────┘
       │
       ▼
┌──────────────────────────────────────┐
│     Knowledge Graph Construction     │
│  - Entity Nodes                      │
│  - Relationship Edges                │
│  - Document Nodes                    │
│  - Chunk Nodes                       │
└──────┬───────────────────────────────┘
       │
       ├──────────────┬─────────────────┐
       ▼              ▼                 ▼
┌────────────┐ ┌────────────┐  ┌──────────────┐
│  Neo4j     │ │ Vector DB  │  │ PostgreSQL   │
│  (Graph)   │ │ (Vectors)  │  │ (Metadata)   │
└────────────┘ └────────────┘  └──────────────┘
```

### Neo4j Schema Design

```cypher
// graph_schema/ontology.cypher

// Node Types
CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:Entity) REQUIRE e.id IS UNIQUE;
CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE;
CREATE CONSTRAINT chunk_id IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE;
CREATE CONSTRAINT concept_id IF NOT EXISTS FOR (co:Concept) REQUIRE co.id IS UNIQUE;

// Indexes for performance
CREATE INDEX entity_name IF NOT EXISTS FOR (e:Entity) ON (e.name);
CREATE INDEX entity_type IF NOT EXISTS FOR (e:Entity) ON (e.type);
CREATE INDEX chunk_embedding IF NOT EXISTS FOR (c:Chunk) ON (c.embedding);

// Full-text search indexes
CALL db.index.fulltext.createNodeIndex(
    "entitySearch",
    ["Entity"],
    ["name", "aliases", "description"]
);

CALL db.index.fulltext.createNodeIndex(
    "chunkSearch",
    ["Chunk"],
    ["text", "summary"]
);

// Graph Schema:
// 
// (Document)-[:CONTAINS]->(Chunk)
// (Chunk)-[:MENTIONS]->(Entity)
// (Entity)-[:RELATED_TO {type, weight}]->(Entity)
// (Entity)-[:INSTANCE_OF]->(Concept)
// (Chunk)-[:REFERENCES]->(Chunk)  // cross-references
// (Entity)-[:CO_OCCURS_WITH {count}]->(Entity)
// (Chunk)-[:NEXT]->(Chunk)  // sequential chunks
// (Chunk)-[:SEMANTICALLY_SIMILAR {score}]->(Chunk)

// Example nodes:
// (:Document {id, title, source, created_at, metadata})
// (:Chunk {id, text, embedding_id, position, summary, token_count})
// (:Entity {id, name, type, aliases, description, importance})
// (:Concept {id, name, domain, definition})
```

### Graph Construction Agent

```python
# agents/graph_extractor_agent.py

import spacy
from typing import List, Dict, Tuple
from neo4j import GraphDatabase
import networkx as nx

class GraphExtractorAgent:
    """
    Extracts entities and relationships to build knowledge graph
    """
    
    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):
        self.nlp = spacy.load("en_core_web_trf")  # Transformer model
        self.driver = GraphDatabase.driver(neo4j_uri, 
                                          auth=(neo4j_user, neo4j_password))
        
        # Add custom NER patterns if needed
        self._add_custom_entity_patterns()
        
    def extract_from_chunks(self, chunks: List[Dict]) -> Dict:
        """
        Extract entities and relationships from chunks
        """
        extraction_results = {
            "entities": [],
            "relationships": [],
            "coreferences": [],
            "graph_stats": {}
        }
        
        # Process chunks in batches
        for i in range(0, len(chunks), 10):
            batch = chunks[i:i+10]
            
            # Extract entities
            entities = self._extract_entities(batch)
            extraction_results["entities"].extend(entities)
            
            # Extract relationships
            relationships = self._extract_relationships(batch, entities)
            extraction_results["relationships"].extend(relationships)
            
            # Resolve coreferences across chunks
            coref = self._resolve_coreferences(batch)
            extraction_results["coreferences"].extend(coref)
        
        # Build graph in Neo4j
        self._build_graph(extraction_results, chunks)
        
        # Compute graph statistics
        extraction_results["graph_stats"] = self._compute_graph_stats()
        
        return extraction_results
    
    def _extract_entities(self, chunks: List[Dict]) -> List[Dict]:
        """
        Named Entity Recognition with entity linking
        """
        entities = []
        
        for chunk in chunks:
            doc = self.nlp(chunk["text"])
            
            for ent in doc.ents:
                entity = {
                    "text": ent.text,
                    "type": ent.label_,
                    "start": ent.start_char,
                    "end": ent.end_char,
                    "chunk_id": chunk["id"],
                    "confidence": self._compute_entity_confidence(ent)
                }
                
                # Entity linking (disambiguate)
                entity["canonical_id"] = self._link_entity(entity)
                
                # Extract entity attributes
                entity["attributes"] = self._extract_entity_attributes(ent, doc)
                
                entities.append(entity)
        
        return entities
    
    def _extract_relationships(self, chunks: List[Dict], 
                              entities: List[Dict]) -> List[Dict]:
        """
        Relationship extraction using dependency parsing and patterns
        """
        relationships = []
        
        for chunk in chunks:
            doc = self.nlp(chunk["text"])
            
            # Method 1: Dependency parsing
            dep_relations = self._extract_from_dependencies(doc, entities)
            relationships.extend(dep_relations)
            
            # Method 2: Pattern matching
            pattern_relations = self._extract_from_patterns(doc, entities)
            relationships.extend(pattern_relations)
            
            # Method 3: Open Information Extraction
            openie_relations = self._extract_with_openie(chunk["text"], entities)
            relationships.extend(openie_relations)
        
        return relationships
    
    def _extract_from_dependencies(self, doc, entities) -> List[Dict]:
        """
        Extract relationships using dependency parse tree
        """
        relationships = []
        
        for token in doc:
            # Subject-Verb-Object patterns
            if token.dep_ in ("nsubj", "nsubjpass"):
                subject = token
                verb = token.head
                
                # Find object
                for child in verb.children:
                    if child.dep_ in ("dobj", "pobj", "attr"):
                        relationships.append({
                            "subject": subject.text,
                            "predicate": verb.lemma_,
                            "object": child.text,
                            "type": "SVO",
                            "confidence": 0.8
                        })
        
        return relationships
    
    def _build_graph(self, extraction_results: Dict, chunks: List[Dict]):
        """
        Build knowledge graph in Neo4j
        """
        with self.driver.session() as session:
            # Create document node
            doc_id = chunks[0]["document_id"]
            session.run("""
                MERGE (d:Document {id: $doc_id})
                SET d.created_at = timestamp()
            """, doc_id=doc_id)
            
            # Create chunk nodes
            for chunk in chunks:
                session.run("""
                    MERGE (c:Chunk {id: $chunk_id})
                    SET c.text = $text,
                        c.position = $position,
                        c.embedding_id = $embedding_id
                    
                    WITH c
                    MATCH (d:Document {id: $doc_id})
                    MERGE (d)-[:CONTAINS]->(c)
                """, 
                chunk_id=chunk["id"],
                text=chunk["text"],
                position=chunk["position"],
                embedding_id=chunk["embedding_id"],
                doc_id=doc_id)
            
            # Create entity nodes and relationships
            for entity in extraction_results["entities"]:
                session.run("""
                    MERGE (e:Entity {id: $entity_id})
                    SET e.name = $name,
                        e.type = $type,
                        e.confidence = $confidence
                    
                    WITH e
                    MATCH (c:Chunk {id: $chunk_id})
                    MERGE (c)-[m:MENTIONS {
                        position: $position
                    }]->(e)
                """,
                entity_id=entity["canonical_id"],
                name=entity["text"],
                type=entity["type"],
                confidence=entity["confidence"],
                chunk_id=entity["chunk_id"],
                position=entity["start"])
            
            # Create entity relationships
            for rel in extraction_results["relationships"]:
                session.run("""
                    MATCH (e1:Entity {name: $subject})
                    MATCH (e2:Entity {name: $object})
                    MERGE (e1)-[r:RELATED_TO {
                        type: $predicate,
                        confidence: $confidence
                    }]->(e2)
                """,
                subject=rel["subject"],
                object=rel["object"],
                predicate=rel["predicate"],
                confidence=rel["confidence"])
            
            # Create chunk sequence edges
            for i in range(len(chunks) - 1):
                session.run("""
                    MATCH (c1:Chunk {id: $chunk1_id})
                    MATCH (c2:Chunk {id: $chunk2_id})
                    MERGE (c1)-[:NEXT]->(c2)
                """,
                chunk1_id=chunks[i]["id"],
                chunk2_id=chunks[i+1]["id"])
            
            # Compute entity co-occurrence
            self._compute_cooccurrence(session)
            
            # Add community detection
            self._detect_communities(session)
    
    def _compute_cooccurrence(self, session):
        """
        Create CO_OCCURS_WITH edges between entities in same chunks
        """
        session.run("""
            MATCH (c:Chunk)-[:MENTIONS]->(e1:Entity)
            MATCH (c)-[:MENTIONS]->(e2:Entity)
            WHERE e1.id < e2.id
            WITH e1, e2, count(DISTINCT c) as count
            MERGE (e1)-[r:CO_OCCURS_WITH]->(e2)
            SET r.count = count,
                r.weight = count * 1.0
        """)
    
    def _detect_communities(self, session):
        """
        Run community detection to find entity clusters
        """
        session.run("""
            CALL gds.graph.project(
                'entityGraph',
                'Entity',
                {
                    RELATED_TO: {orientation: 'UNDIRECTED'},
                    CO_OCCURS_WITH: {orientation: 'UNDIRECTED'}
                }
            )
        """)
        
        session.run("""
            CALL gds.louvain.stream('entityGraph')
            YIELD nodeId, communityId
            WITH gds.util.asNode(nodeId) AS entity, communityId
            SET entity.community = communityId
        """)
```

### Graph-Based Retrieval Agent

```python
# agents/graph_retriever_agent.py

from neo4j import GraphDatabase
from typing import List, Dict, Set

class GraphRetrieverAgent:
    """
    Retrieves relevant information using graph traversal and reasoning
    """
    
    def __init__(self, neo4j_uri, neo4j_user, neo4j_password):
        self.driver = GraphDatabase.driver(neo4j_uri,
                                          auth=(neo4j_user, neo4j_password))
    
    def retrieve(self, query_analysis: Dict, k: int = 10) -> List[Dict]:
        """
        Multi-strategy graph retrieval
        """
        results = []
        
        # Strategy 1: Entity-based retrieval
        if query_analysis.get("entities"):
            entity_results = self._retrieve_by_entities(
                query_analysis["entities"], k)
            results.extend(entity_results)
        
        # Strategy 2: Relationship traversal
        if query_analysis.get("relationships"):
            relation_results = self._retrieve_by_relations(
                query_analysis["relationships"], k)
            results.extend(relation_results)
        
        # Strategy 3: Multi-hop reasoning
        if query_analysis.get("requires_multi_hop"):
            multi_hop_results = self._multi_hop_retrieval(
                query_analysis, k)
            results.extend(multi_hop_results)
        
        # Strategy 4: Community-based retrieval
        community_results = self._retrieve_by_community(
            query_analysis, k)
        results.extend(community_results)
        
        # Deduplicate and rank
        results = self._deduplicate_and_rank(results, query_analysis)
        
        return results[:k]
    
    def _retrieve_by_entities(self, entities: List[str], k: int) -> List[Dict]:
        """
        Retrieve chunks mentioning query entities
        """
        with self.driver.session() as session:
            result = session.run("""
                // Find entities matching query
                UNWIND $entities AS query_entity
                CALL db.index.fulltext.queryNodes('entitySearch', query_entity)
                YIELD node AS e, score
                
                // Get chunks mentioning these entities
                MATCH (c:Chunk)-[m:MENTIONS]->(e)
                
                // Get related entities for context
                OPTIONAL MATCH (e)-[r:RELATED_TO]-(related:Entity)
                
                // Get document context
                MATCH (d:Document)-[:CONTAINS]->(c)
                
                RETURN DISTINCT 
                    c.id AS chunk_id,
                    c.text AS text,
                    collect(DISTINCT e.name) AS entities,
                    collect(DISTINCT {
                        name: related.name,
                        relationship: type(r)
                    }) AS related_entities,
                    d.title AS document_title,
                    score,
                    size((c)-[:MENTIONS]->()) AS entity_density
                ORDER BY score DESC, entity_density DESC
                LIMIT $k
            """, entities=entities, k=k)
            
            return [dict(record) for record in result]
    
    def _retrieve_by_relations(self, relationships: List[Dict], 
                               k: int) -> List[Dict]:
        """
        Retrieve based on relationship patterns
        Example: "What causes X?" -> find (X)-[:CAUSES]->(?)
        """
        with self.driver.session() as session:
            results = []
            
            for rel in relationships:
                result = session.run("""
                    MATCH (e1:Entity)-[r:RELATED_TO]->(e2:Entity)
                    WHERE e1.name CONTAINS $subject 
                      AND r.type = $predicate
                    
                    // Get chunks containing this relationship
                    MATCH (c:Chunk)-[:MENTIONS]->(e1)
                    MATCH (c)-[:MENTIONS]->(e2)
                    
                    RETURN 
                        c.id AS chunk_id,
                        c.text AS text,
                        e1.name AS subject,
                        r.type AS relationship,
                        e2.name AS object,
                        r.confidence AS confidence
                    ORDER BY r.confidence DESC
                    LIMIT $k
                """, 
                subject=rel["subject"],
                predicate=rel["predicate"],
                k=k)
                
                results.extend([dict(record) for record in result])
            
            return results
    
    def _multi_hop_retrieval(self, query_analysis: Dict, 
                            k: int) -> List[Dict]:
        """
        Multi-hop reasoning: traverse graph paths
        Example: "How does A affect C?" -> (A)-[*1..3]->(C)
        """
        with self.driver.session() as session:
            start_entity = query_analysis["start_entity"]
            end_entity = query_analysis["end_entity"]
            max_hops = query_analysis.get("max_hops", 3)
            
            result = session.run("""
                // Find shortest paths between entities
                MATCH path = shortestPath(
                    (start:Entity {name: $start})-[*1..{max_hops}]-(end:Entity {name: $end})
                )
                
                // Extract entities along path
                WITH path, 
                     [node IN nodes(path) | node.name] AS path_entities,
                     [rel IN relationships(path) | type(rel)] AS path_relations
                
                // Find chunks that mention entities in path
                UNWIND path_entities AS entity_name
                MATCH (e:Entity {name: entity_name})<-[:MENTIONS]-(c:Chunk)
                
                // Score based on path relevance
                WITH c, path_entities, path_relations,
                     size([e IN path_entities WHERE (c)-[:MENTIONS]->(:Entity {name: e})]) 
                     AS entities_in_chunk
                
                RETURN DISTINCT
                    c.id AS chunk_id,
                    c.text AS text,
                    path_entities AS reasoning_path,
                    path_relations AS relationship_path,
                    entities_in_chunk AS path_coverage
                ORDER BY path_coverage DESC, size(reasoning_path) ASC
                LIMIT $k
            """.format(max_hops=max_hops),
            start=start_entity,
            end=end_entity,
            k=k)
            
            return [dict(record) for record in result]
    
    def _retrieve_by_community(self, query_analysis: Dict, 
                               k: int) -> List[Dict]:
        """
        Retrieve from entity communities (topic clusters)
        """
        with self.driver.session() as session:
            entities = query_analysis.get("entities", [])
            
            result = session.run("""
                // Find communities of query entities
                UNWIND $entities AS query_entity
                MATCH (e:Entity)
                WHERE e.name CONTAINS query_entity
                
                WITH collect(DISTINCT e.community) AS communities
                
                // Get all entities in these communities
                MATCH (e:Entity)
                WHERE e.community IN communities
                
                // Get chunks mentioning community entities
                MATCH (c:Chunk)-[:MENTIONS]->(e)
                
                WITH c, communities,
                     size((c)-[:MENTIONS]->(:Entity)) AS total_entities,
                     size((c)-[:MENTIONS]->(:Entity {community: communities[0]})) 
                     AS community_entities
                
                RETURN 
                    c.id AS chunk_id,
                    c.text AS text,
                    community_entities * 1.0 / total_entities AS community_density
                ORDER BY community_density DESC
                LIMIT $k
            """, entities=entities, k=k)
            
            return [dict(record) for record in result]
    
    def get_entity_context(self, entity_id: str, depth: int = 2) -> Dict:
        """
        Get rich context around an entity (for context enrichment)
        """
        with self.driver.session() as session:
            result = session.run("""
                MATCH (e:Entity {id: $entity_id})
                
                // Get direct relationships
                OPTIONAL MATCH (e)-[r1:RELATED_TO]-(neighbor:Entity)
                
                // Get second-degree relationships
                OPTIONAL MATCH (neighbor)-[r2:RELATED_TO]-(second_degree:Entity)
                WHERE second_degree.id <> e.id
                
                // Get entity attributes
                OPTIONAL MATCH (e)-[:INSTANCE_OF]->(concept:Concept)
                
                RETURN 
                    e.name AS entity,
                    e.type AS entity_type,
                    collect(DISTINCT {
                        name: neighbor.name,
                        relationship: type(r1),
                        confidence: r1.confidence
                    }) AS direct_neighbors,
                    collect(DISTINCT {
                        name: second_degree.name,
                        relationship: type(r2)
                    })[..10] AS indirect_neighbors,
                    concept.name AS concept
            """, entity_id=entity_id)
            
            return dict(result.single())
```

### Hybrid Retrieval Fusion

```python
# agents/fusion_agent.py

from typing import List, Dict
import numpy as np
from sklearn.preprocessing import MinMaxScaler

class HybridFusionAgent:
    """
    Fuses results from vector, sparse, and graph retrieval
    """
    
    def __init__(self):
        self.scaler = MinMaxScaler()
    
    def fuse(self, 
             vector_results: List[Dict],
             sparse_results: List[Dict],
             graph